\documentclass[dvipdfmx, 11pt, aspectratio=169]{beamer}   % dvipdfmx で非 ASCII 画像も安全
\usetheme{metropolis}
\usecolortheme{metropolis}
\usepackage{booktabs}
\usepackage{ulem}
\usepackage{tabularx}
\newcolumntype{L}{>{\raggedright\arraybackslash}X} % 左寄せの X 列
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{listings}
\usepackage{xcolor}
% コードのハイライト設定
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    framexleftmargin=2em,
    showtabs=false,                  
    tabsize=2
}

\lstdefinelanguage{CUDA}{
  language=[ANSI]C++,                % C++ を継承
  morekeywords={
    __global__,__device__,__shared__,__constant__,__managed__,cudaError_t,
    __syncthreads,atomicAdd,atomicSub,atomicExch,dim3,blockIdx,threadIdx
  }
}

\lstdefinestyle{makefilestyle}{
  language        = make,        % listings 標準の make 言語
  basicstyle      = \ttfamily\footnotesize,
  keywordstyle    = \color{blue}\bfseries,
  commentstyle    = \color{codegreen},
  stringstyle     = \color{codepurple},
  numbers         = left,        % 行番号
  numberstyle     = \scriptsize\color{codegray},
  stepnumber      = 1,
  numbersep       = 6pt,
  tabsize         = 4,           % TAB＝4 スペース
  showstringspaces= false,
  showtabs        = false,
  breaklines      = true,
  morekeywords    = {CC,CXX,LD,AR,CFLAGS,CXXFLAGS,LDFLAGS,RM,\%.o,all,clean}, % 独自キーワード
  xleftmargin     = 2em,         % コードブロック全体を右へ
  frame           = single,      % 枠線
  backgroundcolor = \color{backcolour}
}
%%%%
\setbeamertemplate{section in toc shaded}[default][20]
\makeatletter
\def\beamer@tocaction{\vskip0.1em}
\patchcmd{\beamer@sectionintoc}{\vskip1.5em}{\beamer@tocaction}{}{}
\patchcmd{\beamer@sectionintoc}{\vskip1.5em plus 1fil}{\beamer@tocaction}{}{}
\makeatother
%%%
\usepackage{hyperref}
% URLの#記号をエスケープするための設定
\makeatletter
\g@addto@macro\UrlSpecials{\do\#{\char35 }}
\makeatother
\newcommand{\ulhref}[2]{\href{#1}{\textcolor{cyan}{\uline{#2}}}}
\lstset{style=mystyle}
\title{CUDA: understand to be a genuine user}
\author{takigawa}
\institute{The university of Tokyo, EEIC, Taura Lab}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% タイトルスライド
\begin{frame}
  \titlepage       
\end{frame}
% 目次
\begin{frame}{Contents}
  \begin{enumerate}%[<+->]   % <+-> で 1 行ずつ出現
    \item What is CUDA?: Introduction
    \item A CUDA program for beginners
    \item How CUDA works
    \item Optimize CUDA program
    \item Practice Problems
  \end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{What is CUDA?}
% 目次
% \begin{frame}{Contents}
%   \begin{enumerate}%[<+->]   % <+-> で 1 行ずつ出現
%     \item What is CUDA?: Introduction
%     \item \textcolor{gray}{A CUDA program for beginners}
%     \item \textcolor{gray}{How CUDA works}
%     \item \textcolor{gray}{Optimize CUDA program}
%     \item \textcolor{gray}{Practice Problems}
%   \end{enumerate}
% \end{frame}
\begin{frame}
    \frametitle{Contents}
    \linespread{0.6}\selectfont
    \tableofcontents[currentsection, hideothersubsections]
\end{frame}
%%%%%
\begin{frame}{CUDA is the abstraction of GPU(s) for programmers}
  GPU (Graphical Processing Unit) is a device separated from CPU (Central PU).
  \begin{itemize}
    \item Code that runs on GPU must be designated as a kernel
    \item Data must be copied between CPU and GPU(s)
    \item A GPU is often called a \textcolor{blue}{\textit{"device"}}
    \item A CPU is often called a \textcolor{blue}{\textit{"host"}}
  \end{itemize}
  % figure
  \begin{columns}
    \column{0.45\textwidth}
    \begin{figure}
      \includegraphics[scale=0.2]{img/host.png}
      \caption{Host}
    \end{figure}
    \column{0.55\textwidth}
    \begin{figure}
      \includegraphics[scale=0.15]{img/device.jpg}
      \caption{Device}
    \end{figure}
  \end{columns}
\end{frame}
%%%%%%
\begin{frame}{CUDA is the abstraction of GPU(s) for programmers}
  CUDA is a platform for parallel computing on NVIDIA GPU.
  \begin{itemize}
    \item language extension: C/C++, Fortran
    \item tools: compiler(nvcc), debugger(cuda-gdb), profiler(Nsight Systems)
    \item APIs: Driver API, Runtime API
    \item Libraries: cuBLAS, cuFFT, cuDNN, etc.
  \end{itemize}
  Their common goal is to provide programmers with a "good" abstraction of GPU(s).
  How "good"?: usable, simple, highly affine to hardware(=easy to bring out the performance)
\end{frame}
%%%%%
\begin{frame}[fragile]{To compile/run CUDA programs: NVCC}
  \begin{block}{}
    \begin{lstlisting}
      nvcc -arch=sm_90 programs.cu%
\end{lstlisting}
  \end{block}
  \begin{itemize}
    \item compile with \lstinline|nvcc| command
    \item the natural extension of CUDA program is \lstinline|.cu|
    \item \lstinline|-arch| flag designates GPU Architecture
    \begin{itemize}
      \item \lstinline|compute_XX|: Virtual architecture
      \item \lstinline|sm_XX|: Physical architecture (SM generation)
    \end{itemize}
  \end{itemize}
\end{frame}
%%%%%%
\begin{frame}[fragile]{Interlude: How to know the proper architechture}
  Use \lstinline|cudaGetDeviceXXXXX| APIs and get device query.
  \begin{block}{\lstinline|device_query.cu|}
    \begin{lstlisting}[language=CUDA]
#include <cuda_runtime.h>
int deviceCount;
cudaError_t error = cudaGetDeviceCount(&deviceCount);
cudaDeviceProp deviceProp;
cudaGetDeviceProperties(&deviceProp, i);
      
printf("Device %d: %s\n", i, deviceProp.name);
printf("  Compute capability: %d.%d\n", deviceProp.major, deviceProp.minor);
printf("  Total global memory: %.2f GB\n", deviceProp.totalGlobalMem / (1024.0 * 1024.0 * 1024.0));
\end{lstlisting}
  \end{block}\vspace{-\baselineskip}
  or \vspace{-1.5\baselineskip}
  \begin{block}{}
    \begin{lstlisting}[language=bash]
  nvidia-smi --query-gpu=compute_cap
\end{lstlisting}
  \end{block}\vspace{-\baselineskip}
  The architecture for GH200 is \lstinline|sm_90|.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A CUDA program for beginners}
% 目次
% \begin{frame}{Contents}
%   \begin{enumerate}%[<+->]   % <+-> で 1 行ずつ出現
%     \item \textcolor{gray}{What is CUDA?: Introduction}
%     \item A CUDA program for beginners
%     \begin{enumerate}
%       \item Kernels; writing and launching
%       % __global__ kernelFunction, kernelFunction<<nb, bs>>(arg1, arg2, ...)
%       \item Host-Device Data Communication (+ Synchronization)
%       % cudaMalloc, cudaMemcpy, cudaMallocManaged, cudaDeviceSynchronize
%     \end{enumerate}
%     \item \textcolor{gray}{How CUDA works}
%     \item \textcolor{gray}{Optimize CUDA program}
%     \item \textcolor{gray}{Practice Problems}
%   \end{enumerate}
% \end{frame}
\begin{frame}
    \frametitle{Contents}
    \linespread{0.6}\selectfont
    \tableofcontents[currentsection, hideothersubsections]
\end{frame}
%%%%%%%%%%
\subsection{Setting environment}
%%%%%
\begin{frame}[fragile]{Setting environment: Using GH200(s) on miyabi}
  \begin{enumerate}
    \item \url{https://miyabi-www.jcahpc.jp/login}にアクセスし、パスワード初期化を選択する
    \item 指示にしたがい、 Miyabi利用支援ポータルにアクセスする
    \item ドキュメント閲覧/Miyabiシステム利用手引書 をダウンロードする (Strongly recommended)
    \item 手引書のP.9 システム初回ログイン時の設定 の手順を完了する
    \item 手引書のP.22 SSHログイン/初回ログイン の手順を完了する (必ず緊急用スクラッチコードを控えること)
    \item (必要に応じて) エディタから2回目ログインを行う
  \end{enumerate}
  2要素認証が必須である。

  【注意】 ログインノード \lstinline|/home/cXXXXX|ではなく、計算ノード\lstinline|/work/gc64/cXXXXX|で作業する
\end{frame}
%%%%%%%%%%
\subsection{hello\_world}
%%%%%
\begin{frame}
    \frametitle{Contents}
    \linespread{0.6}\selectfont
    \tableofcontents[currentsection, hideothersubsections]
\end{frame}
%%%%%
\begin{frame}[fragile]{Sample program \#1: \texttt{hello\_world.cu}}
  See \url{https://github.com/gunnersgoestocl/cuda-introduction/tree/main/setup} for more information.
\begin{block}{}
  \begin{lstlisting}[language=CUDA]
#include <stdio.h>
#include <cuda_runtime.h>             // for Runtime APIs

__global__ void hello(){              // kernel function
    printf("Hello CUDA World !!\n");
}

int main() {
    hello<<< 2, 4 >>>();              // launch kernel
    cudaDeviceSynchronize();          // wait until kernel completes
    return 0;
}
  \end{lstlisting}
\end{block}
\end{frame}
%%%%%%
\begin{frame}[fragile]{3 files are required for execution on miyabi}
% .cu, makefile, .sh
  \begin{itemize}
    \item \lstinline|.cu| file: CUDA user program
    \item \lstinline|makefile|: compile and clean
    \item shell script: to submit batch job, see official docs for more info
  \end{itemize}
  \begin{columns}
    \column{0.5\textwidth}
    \begin{lstlisting}[style=makefilestyle, basicstyle=\ttfamily\tiny]
NVCC := nvcc
NVCCFLAGS := -arch=sm_90 -O3
# .cu ファイルから実行ファイルを生成
CUDA_EXECUTABLES := $(patsubst %.cu,%,$(wildcard *.cu))
# デフォルトのターゲット
all: $(C_EXECUTABLES) $(CUDA_EXECUTABLES)
# .cu ファイルから実行ファイルを生成
%: %.cu
	$(NVCC) $(NVCCFLAGS) $< -o $@
# clean ターゲットの定義
.PHONY: clean
clean:
	rm -f $(CUDA_EXECUTABLES)
\end{lstlisting}
    \column{0.5\textwidth}
    \begin{lstlisting}[language=sh]
#!/bin/bash
#PBS -q debug-g
#PBS -l select=1
#PBS -W group_list=gc64
#PBS -j oe

module purge
module load cuda

cd ${PBS_O_WORKDIR}
./a.out 256
\end{lstlisting}
  \end{columns}
\end{frame}
%%%%%%
\begin{frame}[fragile]{Contents of .cu file: kernel function}
% keywords (__global__ etc.)
  \begin{itemize}
    \item \textcolor{blue}{"kernel"} (sometimes \textcolor{blue}{"GPU kernel"}): A function that runs on GPU
    \begin{itemize}
      \item SYNTAX: An ordinary C/C++ function that returns nothing (\lstinline|void|)
      \item SYNTAX: Add \textcolor{blue}{\lstinline|__global__|} keyword beforehand
    \end{itemize}
  \end{itemize}
  \begin{block}{}
    \begin{lstlisting}[caption=kernel template, language=CUDA]
__global__ void f(...args...) { ...body... }
    \end{lstlisting}
  \end{block}
\end{frame}
%%%%%%
\begin{frame}[fragile]{Contents of .cu file: launching kernel by a host}
% specify the number of threads (and blocks), you can use dim3 struct
  A host (CPU) launches a kernel to devices.
  \begin{itemize}
    \item Programmers must specify the number of threads by \lstinline|<<nb, bs>>|
    \begin{itemize}
      \item \lstinline|nb|: Number of Blocks (per grid) (sometimes \lstinline|gridDim|)
      \item \lstinline|bs|: Block Size (sometimes \lstinline|blockDim|)
    \end{itemize}
    \item \lstinline|nb * bs| is the number of CUDA threads created
  \end{itemize}\vspace{-1.5\baselineskip}
  \begin{block}{}
    \begin{lstlisting}[language=CUDA]
      // ... code run on host ...
      
      f<<gridDim, blockDim>>(...args...);
\end{lstlisting}
  \end{block}\vspace{-\baselineskip}
  \begin{itemize}
    \item \lstinline|nb, bs| can be ${1,2,3}$-Dimensional using type \lstinline|dim3|
  \end{itemize}\vspace{-1.5\baselineskip}
  \begin{block}{}
    \begin{lstlisting}[language=CUDA]
    dim3 block(x_threads_block, y_threads_block);    // x, y(, z)
    dim3 grid(x_blocks_grid, z_blocks_grid);         // x, y(, z)
          
    f<<grid, block>>(...args...);
\end{lstlisting}
  \end{block}
\end{frame}
%%%%%%
\begin{frame}[fragile]{Interlude: register values programs can explicitly use}
% gridDim, blockIdx, blockDim, threadIdx
\begin{itemize}
  \item Threads which executes a single instruction can be executed in parallel.
  \item So, programmers are expected to make the kernel visible to all threads as the same instruction.
  \item For this perspective, a unique ID of each thread (= the loop index) is the key.
\end{itemize}
In CUDA, each kernel can access its own thread ID through built-in variables on the \ulhref{https://docs.nvidia.com/cuda/parallel-thread-execution/\#special-registers}{Special Registers}.
\begin{itemize}
  \item \lstinline|blockDim.{x,y} = bs| (the block size)
  \item \lstinline|gridDim.{x,y} = nb| (the number of blocks)
  \item \lstinline|threadIdx.{x,y} = | the thread ID within a block $(\in [0, bs))$
  \item \lstinline|blockIdx.{x,y} = | the thread's block ID $(\in [0, nb))$
  \item[->] \lstinline|blockDim.x * blockIdx.x + threadIdx.x| could be the loop index
\end{itemize}
\end{frame}
%%%%%%%%
\subsection{hello\_gpu}
%%%%%
\begin{frame}
    \frametitle{Contents}
    \linespread{0.6}\selectfont
    \tableofcontents[currentsection, hideothersubsections]
\end{frame}
%%%%%
\begin{frame}[fragile]{Sample program \#2: \texttt{hello\_gpu.cu}}
\begin{block}{}\vspace{-\baselineskip}
  \begin{lstlisting}[language=CUDA, basicstyle=\ttfamily\tiny]
#include <stdio.h>
#include <cuda_runtime.h>

__device__ void gpuAdd(int *number){  *number += 1; }
__global__ void callGpu(int *number){  gpuAdd(number); }

int main(){
  int device_id = 0;  cudaSetDevice(device_id); //device set up
  int *a = (int*)malloc(sizeof(int));  *a = 0;  //allocate memory on host(cpu)
  int *a_dev = 0;  cudaMalloc((void**)&a_dev, sizeof(int)); // allocate memory on gpu
  cudaMemcpy(a_dev, a, sizeof(int), cudaMemcpyHostToDevice);  // memcpy host -> device
  // execute
  callGpu<<<1, 1>>>(a_dev);
  cudaDeviceSynchronize(); // Wait until GPU processing finishs.
  
  cudaMemcpy(a, a_dev, sizeof(int), cudaMemcpyDeviceToHost);  // memcpy device -> host 
  cudaFree(a_dev);  // free

  printf("ans: %d \n", *g); return 0; // display the answer
}
\end{lstlisting}
\end{block}
\end{frame}
%%%%%
\begin{frame}{Data communication between H \& Ds: overview}
% cudaMalloc, cudaMemcpy
  \begin{itemize}
    \item Host memory and device memory are (basically) \textcolor{red}{\textit{separete}}
    \item The device(D) cannot access data on the host(H) and vice versa by hardware
    \begin{itemize}
      \item Access to another memory (including that of another device) causes \lstinline|Segfault|
    \end{itemize}
    \item Software need to explicitly specify when and what to communicate between H \& D
  \end{itemize}
\end{frame}
%%%%%
\begin{frame}[fragile]{Data communication between H \& Ds: template to send}
  \begin{enumerate}[<+->]
    \item allocate data of the same size both on host and device
\begin{lstlisting}[language=CUDA]
size_t sz = sizeof(int)*len;
int *a = (int*)malloc(sz);
int *a_dev = 0;  cudaMalloc((void**)&a_dev, sz);
// (void**)&a_dev is the address of the pointer variable (a_dev)
// Head address of GPU global memory is written to a_dev
\end{lstlisting}
    \item the host works on the kinda initialization of the host data
\begin{lstlisting} [language=C]
for ( ... ) { a[i] = ... } // on host, initialize input data of kernel
\end{lstlisting}
    \item copy the data to the device
\begin{lstlisting} [language=CUDA]
cudaMemcpy(a_dev, a, sz, cudaMemcpyHostToDevice); // target address, source address, size, keyword
\end{lstlisting}
    \item pass the device pointer to the kernel
\begin{lstlisting}[language=CUDA]
f<<nb, bs>>(a_dev, ...);
\end{lstlisting}
    \end{enumerate}
\end{frame}
%%%%%
\begin{frame}[fragile]{Data communication between H \& Ds: template to retrieve}
  \begin{enumerate}[<+->]
    \item allocate data of the same size both on host and device
\begin{lstlisting}[language=CUDA]
size_t sz = sizeof(int)*len;
int *r = (int*)malloc(sz);                        // host memory
int *r_dev = 0;  cudaMalloc((void**)&r_dev, sz);  // device memory
\end{lstlisting}
    \item pass the device pointer of receptacle to the kernel
\begin{lstlisting}[language=CUDA]
f<<nb, bs>>(...,r_dev, ...);  // args must include pointer(s) of input and output
\end{lstlisting}
    \item copy the data to the device
\begin{lstlisting} [language=CUDA]
cudaMemcpy(r, r_dev, sz, cudaMemcpyDeviceToHost); // target address, source address, size, keyword
\end{lstlisting}
    \end{enumerate}
\end{frame}
%%%%%
\begin{frame}[fragile]{Host-Device synchronization}
  \begin{itemize}
    \item A kernel call and the host overlap.
    \item Multiple kernel calls are serialized on the GPU side, by default (Host basically cannot control)
    \begin{itemize}
      \item \textcolor{blue}{Grid} is an abstraction of a one time launch of the kernel.
      \item Grid is managed using a data struct FIFO queue called \textcolor{blue}{Stream}
      \item If you want to execute multiple kernel calls concurrently, you must use multiple Streams or Devices.
    \end{itemize}
    \item \lstinline|cudaDeviceSynchronize()| is an API to wait for the kernel to finish
  \end{itemize}
  \begin{columns}
    \column{0.4\textwidth}
    \begin{lstlisting}[language=CUDA]
h0();
g0<<...,...>>();
h1();
g1<<...,...>>();
cudaDeviceSynchronize();
h2();
\end{lstlisting}
    \column{0.6\textwidth}
    \begin{itemize}
      \item \lstinline|g0| might overlap with \lstinline|h1|
      \item \lstinline|g0| and \lstinline|g1| do not overlap because they are assigned to the same stream 0
      \item \lstinline|h2|s does not overlap with anything because of \lstinline|cudaDeviceSynchronize()|
    \end{itemize}
  \end{columns}
\end{frame}
%%%%%%%%
\begin{frame}{(+) "Programmer-free" data communication between H \& Ds}
% cudaMallocManaged, Unified Memory
  \begin{itemize}[<+->]
    \item Recent NVIDIA GPUs support \textcolor{blue}{Unified Memory} that \textbf{eliminate} the need for 
    \begin{itemize}
      \item explicit data movement between host and device memory
      \item dual pointer management
    \end{itemize}
    \item \textcolor{blue}{cudaMallocManaged} is an API that hides the copy of data between the host and devices
    \begin{itemize}
      \item The CPU and GPU page tables are linked at the unified virtual address, and a hardware page fault fires the moment a GPU/CPU accesses a page.
      \item The moment a GPU/CPU accesses a page, a hardware page fault fires and the page is moved on-demand.
      \item Coherency is ensured at the kernel synchronization point.
    \end{itemize}
  \end{itemize}
  Sample Code: \texttt{vecadd}
\end{frame}
%%%%%
\begin{frame}[fragile]{Conventional vs Unified Memory}
  \begin{table}
    \centering
    {\footnotesize
      \begin{tabularx}{\textwidth}{@{}l L L@{}}
        Purpose & Conventional (\lstinline|cudaMalloc|+\lstinline|cudaMemcpy|) & Unified Memory (\lstinline|cudaMallocManaged|)\\\midrule[1pt]
        Memory Management & allocates \textbf{separate buffers} for host and device, and makes copy explicit & \textbf{single pointer} can be referenced from either CPU/GPU\\\midrule
        copy & All buffer transfers each time \lstinline|cudaMemcpy()| is called & \textbf{automatic transfer per page} (on demand) \\\midrule
        address space & different values for CPU and GPU & \textbf{Unified Virtual Address (UVA)}-share same value \\\midrule
        oversubscribe & impossible & GPU Can allocate more than the memory capacity and swap unused pages to the host \\\midrule
        Optimization API & None & Manual tuning of placement with \lstinline|cudaMemPrefetchAsync|, \lstinline|cudaMemAdvise|\\
        \bottomrule
      \end{tabularx}
    }
  \end{table}
\end{frame}
%%%%%
% \begin{frame}{What happens behind \texttt{cudaMallocManaged}?}
%   \begin{columns}[T]
%     \begin{column}{0.47\textwidth}
%       \begin{enumerate}[<+->, series=cudaMallocManaged]
%         \item Allocation
        
%         Driver 
%         \item
%       \end{enumerate}
%     \end{column}
%     \column{0.01\textwidth}
%     \centering\vrule width 0.4pt
%     \begin{column}{0.47\textwidth}
%       \begin{enumerate}[<+->, series=cudaMallocManaged, resume]
%         \item 
%       \end{enumerate}
%     \end{column}
% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{HOW CUDA works}
% 目次
% \begin{frame}{Contents}
%   \begin{enumerate}%[<+->]   % <+-> で 1 行ずつ出現
%     \item \textcolor{gray}{What is CUDA?: Introduction}
%     \item \textcolor{gray}{A CUDA program for beginners}
%     \item How CUDA works
%     \begin{enumerate}
%       \item Architecture of NVIDIA GPU
%       % GPU unit, global memory, stream processor
%       \item Grid, block, thread; abstractions by CUDA
%       % block-SM, grid-GPU unit
%       \item Warp; Parallel Thread eXecution
%       % warp consists of 32 threads, SIMD, why threadIdx/blockIdx
%       \item Stream: beyond Grid
%       % stream is a FIFO queue, stream ID
%       \item Memory Hierarchy in CUDA
%       % global memory, shared memory, cache
%       \item Resolving race condition on CUDA
%       % atomicAdd, barrier synchronization, reduction(, cooperative groups)
%     \end{enumerate}
%     \item \textcolor{gray}{Optimize CUDA program}
%     \item \textcolor{gray}{Practice Problems}
%   \end{enumerate}
% \end{frame}
\begin{frame}
    \frametitle{Contents}
    \linespread{0.6}\selectfont
    \tableofcontents[currentsection, hideothersubsections]
\end{frame}
%%%%%%%%%
\subsection{Architecture of NVIDIA GPU}
\begin{frame}{Architecture of NVIDIA GPU: GPU unit}
% global memory, SM, link etc.
\vspace{-\baselineskip}
\begin{columns}
  \column{0.45\textwidth}
  \begin{figure}
    \includegraphics[scale=0.07]{img/gpuUnit.png}
    \caption{Device GPU}
  \end{figure}
  \column{0.49\textwidth}
  {\footnotesize
  \begin{itemize}
    \item GBM3 Memory
    
    known as \textcolor{blue}{global memory}, which has large capacity but slow access speed
    \item \textcolor{blue}{Stream Multiprocessor (SM)}
    
    In charge of multiple blocks, performs the operations that make up the kernel in parallel.
    \item Host Interface \& Command Processing

    Interface to communicate with the host CPU, and a command processing unit that manages the execution of commands.
  \end{itemize}
  }
\end{columns}
\end{frame}
%%%%%
\begin{frame}{Architecture of NVIDIA GPU: Stream Multiprocessors}
% CUDA core, regfile, Warp scheduler
\vspace{-\baselineskip}
\begin{columns}
  \column{0.47\textwidth}
  \vspace{\baselineskip}
  \begin{figure}
    \includegraphics[scale=0.07]{img/sm90.png}
    \caption{Device GPU}
  \end{figure}
  \column{0.47\textwidth}
  {\footnotesize
  \begin{itemize}
    \item \textcolor{blue}{shared memory}
    
    has small capacity but relatively fast access speed
    \item \textcolor{blue}{Warp scheduler}

    Manage parallel execution on CUDA cores in units of Warps that comprise blocks allocated to the SM
    \item \textcolor{blue}{CUDA core}: The core that performs the actual computation
    \item \textcolor{blue}{Tensor core}: The core specialized for matrix operations
  \end{itemize}
  }
\end{columns}
\end{frame}
% %%%%%
% \begin{frame}{Architecture of NVIDIA GPU: Stream Multiprocessors}
% % cache, shared memory

% \end{frame}
%%%%%%%%
\subsection{Grid, block, thread; abstractions by CUDA}
%%%%%
\begin{frame}
    \frametitle{Contents}
    \linespread{0.6}\selectfont
    \tableofcontents[currentsection, hideothersubsections]
\end{frame}
%%%%%
\begin{frame}{3 easy pieces about hardware - software}
Programmers know 3 things about CUDA: Grid, Block, Thread
  \begin{itemize}
    \item (Review) \textcolor{blue}{Grid} corresponds to a one time launch of the kernel.
    \begin{itemize}
      \item A \textcolor{blue}{Grid} is assigned to a single GPU unit (i.e., a single device)
      \item (Review) \textcolor{blue}{Grid} is a collection of \textcolor{blue}{Blocks}
    \end{itemize}
    \item A \textcolor{blue}{Block} is the unit of dispatching to an SM
    \begin{itemize}
      \item A \textcolor{blue}{Block} is assigned to a single SM 
      
      i.e., once a block starts running, it stays on the SM (= occupies registers and shared memory) all the way until it finishes
      \item (Review) A \textcolor{blue}{Block} is a collection of \textcolor{blue}{Threads}
    \end{itemize}
    \item A \textcolor{blue}{Thread} is the smallest unit of execution
    \item A \textcolor{blue}{Thread} is assigned to a single CUDA core
  \end{itemize}
\end{frame}
%%%%%
\begin{frame}{Motivation: You may have questions like ...}
% 3 key questions to understand CUDA abstraction
  \begin{enumerate}
    \item Is it allowed for a \lstinline|block| to have \textcolor{red}{more \lstinline|thread|s than the number of \textbf{processors(CUDA cores)}} in the allocated \textbf{SM}, and if so, how is this handled?
    \item Is it allowed to have \textcolor{red}{more \lstinline|block|s than the number of \textbf{SM}s} that make up the \textbf{GPU unit} corresponding to the \lstinline|grid|, and if so, how is this handled?
    \item How is the execution across \textcolor{red}{multiple \lstinline|grid|s} handled?
  \end{enumerate}
  Hints:
  \begin{itemize}
    \item \lstinline|thread|s belonging to the same \lstinline|block| are executed by the same \textbf{SM}, this does \textcolor{red}{NOT} mean they are executed in parallel. 
    \item A \textbf{SM} is assigned to a \lstinline|block|, but this does \textcolor{red}{NOT} mean that an \textbf{SM} can only be in charge of one \lstinline|block|.
  \end{itemize}
\end{frame}
%%%%%%%%
\subsection{Warp: Parallel Thread eXecution}
%%%%%
\begin{frame}
    \frametitle{Contents}
    \linespread{0.6}\selectfont
    \tableofcontents[currentsection, hideothersubsections]
\end{frame}
%%%%%
\begin{frame}{Warp: The way to realize "Parallel" Thread eXecution}
 % Parallelism in your program is sometimes helped by Concurrency
  \begin{itemize}
    \item The unit of \textcolor{blue}{instruction execution} in the SM is a \textcolor{blue}{\textbf{Warp}}
    \item The number of threads that make up a warp has always been \textbf{32}.
    \item Each thread in a warp shares an instruction pointer (i.e, executes the same instruction at the same time).
    \item The warp scheduler selects a warp from the ready queue (Warp pool) and executes the instruction.
    \item The warp lanes (threads) that is not running does NOT necessarily mean they are saved to memory and swept out from the RF
    \begin{itemize}
      \item because RF of each SM is way larger than that of CPU
      \item the cost of "context switch" (i.e., interrupt a warp and run another) is fundamentally smaller
    \end{itemize}
  \end{itemize}
\end{frame}
%%%%%
\begin{frame}{Warp management: overview}
  \begin{figure}
    \includegraphics[scale=0.075]{img/warpScheduling.png}
  \end{figure}
\end{frame}
%%%%%
\begin{frame}{Hierarchy within an SM}
  % thread < warp < block < SM
  Parallelism within an Stream Multiprocessor consists of three levels.

  \textcolor{blue}{
  thread $\subset$ warp $\subset$ block $\subset$ SM 
  }
  \begin{columns}
    \column{0.60\textwidth}
    \begin{figure}
      \includegraphics[scale=0.075]{img/smHierarchy.png}
    \end{figure}
    \column{0.45\textwidth}
    \begin{itemize}
      \item (recap) A group of \textcolor{blue}{$32$} CUDA threads makes a \textcolor{blue}{warp}
      \item A group of \textcolor{blue}{$bs/32$} warps makes a \textcolor{blue}{block}
      \item There are multiple blocks active on a single SM
    \end{itemize}
  \end{columns}
\end{frame}
%%%%%
\begin{frame}{Advantages of larger number of threads per block, blocks: \textbf{increase Occupancy}}
\begin{itemize}
  \item One of the target is to ensure that CUDA cores are \textcolor{blue}{always occupied by warps}
  \begin{itemize}
    \item Data dependencies or synchronization often make warps inactive(waiting)
    \item Thus, we make sure that (at least) one of the warps are active on every SM by \textcolor{red}{setting the number of threads per block to several times the number of CUDA cores}
  \end{itemize}
  \item Another target is to use all the available SMs in a balanced manner (i.e., SMs are \textcolor{blue}{always occupied} by blocks)
  \begin{itemize}
    \item To fulfill this goal, \textcolor{red}{several times greater number of thread blocks than that of SMs} are required.
  \end{itemize}
  \item In order to know the best GridSize and blockSize, CUDA offers a tool \lstinline|cudaOccupancyMaxPotentialBlockSize(&(int)minGridSize, &(int)blockSize, kernel, 0, 0);|
\end{itemize}
\end{frame}
%%%%%
\begin{frame}{Limitation of Hardware: performance degradation}
However, too many threads per block or blocks are sometimes problematic
\begin{itemize}
  \item If each thread uses large amount of registers, too many threads per block can raise register pressure
  \begin{itemize}
    \item \textbf{Register Spill} costs performance
    \item For example, using \textbf{Tensor Cores} raises register pressure
  \end{itemize}
  \item If each thread block accesses large data, too many thread blocks per SM can raise Cache pressure
  \begin{itemize}
    \item Retire from L1 cache (SM specific memory) will occur too often
    \item Thus, the execution of block will pay the cost of global memory access too often
  \end{itemize}
\end{itemize}
\end{frame}
%%%%%%%%
\subsection{Stream: beyond Grid} % How is the execution across \textcolor{red}{multiple \lstinline|grid|s} handled?
%%%%%%
\begin{frame}
    \frametitle{Contents}
    \linespread{0.6}\selectfont
    \tableofcontents[currentsection, hideothersubsections]
\end{frame}
%%%%%%
\begin{frame}{Stream}
  (review) As long as multiple kernels are submitted to the same stream(i.e., the default stream), 
  they are always executed in series on the GPU side.

  $\because$ If program doesn't specify Stream when launching a kernel, the grid is automatically assigned to \textcolor{blue}{legacy stream 0}.
  \begin{itemize}
    \item \textcolor{blue}{Stream} is a \textbf{FIFO queue} that binds the sequence of operations passed by the host to the GPU.
  \begin{quote}
    \vspace{0.2\baselineskip}
    \small
    The host runtime writes the operation to the command buffer and queues up entries with the same \textcolor{blue}{stream ID}
    ; the GPU grabs the queue on \textcolor{purple}{doorbell notification} and distributes it to the hardware engine (Device), keeping each stream in order.
  \end{quote}\vspace{-\baselineskip}
  \begin{itemize}
    \item \textcolor{blue}{Grid} (= a task) is assigned to a Stream.
    \item If program assign grids to multiple Streams (= launch kernels), GPU firmware pop an item from the \textcolor{red}{available} queue with the \textcolor{red}{highest priority}
  \end{itemize}
    \item Scheduling policy between Streams is a complete \textbf{blackbox}.
  \end{itemize}
\end{frame}
%%%%%
\begin{frame}{Flow of Stream Operation}
  \begin{figure}
    \includegraphics[scale=0.08]{img/grid-advanced_ver2.png}
  \end{figure}
\end{frame}
%%%%%
\begin{frame}[fragile]{Interlude: template to use multiple Streams}
  \begin{lstlisting}[language=CUDA]
cudaStream_t sCompute, sCopy;
cudaStreamCreate(&sCompute);
cudaStreamCreate(&sCopy);

// 1) 非同期コピー（H→D）を stream sCopy
cudaMemcpyAsync(d_in,  h_in,  bytes, cudaMemcpyHostToDevice, sCopy);

// 2) カーネルを stream sCompute
myKernel<<<grid, block, 0, sCompute>>>(d_in, d_out);  // 0: sharedMem ID, 

// 3) 非同期コピー（D→H）を stream sCopy
cudaMemcpyAsync(h_out, d_out, bytes, cudaMemcpyDeviceToHost, sCopy);

// 4) 任意の同期点
cudaEvent_t done;  cudaEventCreate(&done);
cudaEventRecord(done, sCopy);           // sCopy 完了後に立つ
cudaStreamWaitEvent(sCompute, done, 0); // sCompute は done まで待つ
\end{lstlisting}
\end{frame}
%%%%%
\begin{frame}[fragile]{Observation of multiple stream}
\begin{figure}
  \includegraphics[scale=0.25]{img/resultMatmulMultistream20250703142638TimelineDiagram.png}
  \caption{Timeline Diagram of Matmul using multiple streams}
\end{figure}
\vspace{-\baselineskip}

\ulhref{https://github.com/gunnersgoestocl/cuda-introduction/blob/main/multistream/}{Code}  
{\small
  You can do \lstinline|qsub ./run_matmul.sh|, and then \lstinline|./vis.sh YYYYMMDDHHMMSS| to visualize (Timestamp is available from the name of  \lstinline|result/result_**.txt|)}
\end{frame}
%%%%%
\begin{frame}{(+) Using multiple Devices in a single Node : thread}
  † You CANNOT try this on Miyabi, because each node has only \textcolor{red}{1} device.

  \ulhref{https://github.com/gunnersgoestocl/cuda-introduction/blob/main/multinode/matmul_multidevice.cu}{Sample Codes}

  However, I don't have the environment where the program can execute.
\end{frame}
%%%%%
\begin{frame}[fragile]{(+) Using multiple nodes: MPI}
  Simple Idea:
  \begin{itemize}
    \item Create a dedicated MPI process for each GPU, and process all communications with MPI
    \item \textcolor{blue}{Root process}: Initialization, Result presentation
    \begin{itemize}
      \item Activate communication, get process ID (\textcolor{blue}{rank}), set gpu device
      \begin{itemize}
        \item \lstinline|MPI_Init, MPI_Comm_rank, MPI_Comm_size|
      \end{itemize}
      \item Distribute data from \textcolor{blue}{Root} to the processes
      \item Gather result from the processes to \textcolor{blue}{Root}
    \end{itemize}
  \end{itemize}
  Sample Program: 
  \ulhref{https://github.com/gunnersgoestocl/cuda-introduction/blob/main/multinode/matmul_multinode.cu}{\lstinline|matmul_multinode.cu| (github link)} (just \lstinline|make|) 

  Job Script:
  \ulhref{https://github.com/gunnersgoestocl/cuda-introduction/blob/main/multinode/run.sh}{Sample Script (github link)} (just \lstinline|qsub ./run.sh|)
\end{frame}
%%%%%%%%%
\subsection{Memory Hierarchy in CUDA}
%%%%%
\begin{frame}
    \frametitle{Contents}
    \linespread{0.6}\selectfont
    \tableofcontents[currentsection, hideothersubsections]
\end{frame}
%%%%%
\begin{frame}{Memory hierarchy of NVIDIA GPU: overview}
  % global memory, shared memory, cache etc. Clarify which programmers can explicitly use
  \begin{figure}
    \includegraphics[scale=0.105]{img/memHierarchy.png}
  \end{figure}
\end{frame}
%%%%%
\begin{frame}{Memory hierarchy of NVIDIA GPU: features}
  \begin{enumerate}
    \item host storage: PCIe-5, bandwidth is 512GB/s
    \item host memory: up to 480GB, bandwidth is up to 500GB/s
    \item device memory: off-chip, latency is 200-1000 cycles
    \begin{itemize}
      \item local mem (512KB), constant mem (64KB), texture mem (28-256KB), global memory (rest)
      \item HBM3 (96GB), bandwidth is  up to 4.02TB/s, NV Chip2Chip interconnect 900GB/s (bidirectional)
    \end{itemize}
    \item L2 Data Cache: on-chip, shared across all SMs, latency is about 200 cycles
    \item L1 Cache / Shared memory: on-chip, private to each SM, latency is 20-30 cycles, small (total 256KB)
    \begin{itemize}
      \item \textcolor{blue}{Shared memory} is equivalent to a \textbf{user-managed cache} (up to 228KB)
      \item L1 Cache is the space unused as shared memory, buffering when R/W data from/to L2 cache (\textbf{hardware-managed})
    \end{itemize}
    \item \textcolor{blue}{Registers}: private to a thread, R/W from/to L1 or shared memory of its SM
  \end{enumerate}
\end{frame}
%%%%%
\begin{frame}[fragile]{To reduce bottlenecks : coalescing memory access}
  \begin{itemize}
    \item DRAM (global memory) transaction size is \textcolor{blue}{$32$}bytes (i.e., \textcolor{blue}{Load/Store Unit} handles read/write in \textcolor{blue}{$32$}-byte units with \lstinline|write-enable| bits)
    \begin{itemize}
      \item Reccomendation: refer to appendix (Hardware implementation of CPU's LSU)
    \end{itemize}
    \item Accesses to the same transaction unit \uwave{from a single \textbf{warp}} are \textcolor{red}{Coalesced} (i.e., \textcolor{blue}{LSU} handles those \textcolor{red}{logical} accesses as a single \textcolor{red}{physical} access)
    \begin{itemize}
      \item If \lstinline|size_of(item)=|$2^{K}$, access to $2^{5-K}$ items (=$2^{5-K}$ threads) can be coalesced at most
    \end{itemize}
    \item Memory addresses accessed by threads of \textbf{warp} (consists of 32 threads whose \lstinline|Id.x| are consecutive) must be \textcolor{red}{serial and dense}
  \end{itemize}
\end{frame}
%%%%%
\begin{frame}{To reduce bottlenecks : coalescing memory access}
  \begin{columns}
    \column{0.4\textwidth}
    \begin{figure}
      \includegraphics[scale=0.1]{img/coalesced.png}
    \end{figure}
    \column{0.6\textwidth}
    \begin{figure}
      \includegraphics[scale=0.1]{img/non-coalesced.png}
    \end{figure}
  \end{columns}
\end{frame}
%%%%%
\begin{frame}[fragile]{Coalesced mem access vs non-coalesced mem access: Code Example}
  In the following code of \textbf{transpose}, reading from DRAM is coalesced, but writing to DRAM is \textcolor{blue}{not} coalesced
  \begin{columns}
    \column{0.43\textwidth}
\begin{lstlisting}[language=CUDA, basicstyle=\ttfamily\tiny]
template <typename T>
__global__ void transpose_read_coalesced(
  T* output_matrix, 
  T const* input_matrix,
  size_t M, size_t N){
  size_t const j{
    threadIdx.x + blockIdx.x * blockDim.x};
  size_t const i{
    threadIdx.y + blockIdx.y * blockDim.y};
  size_t const from_idx{i * N + j};
  if ((i < M) && (j < N)) {
    size_t const to_idx{j * M + i};
    output_matrix[to_idx] 
      = input_matrix[from_idx];
  }
}
\end{lstlisting}
    \column{0.6\textwidth}
    \includegraphics[scale=0.1]{img/transposeReadCoalesced.png}
  \end{columns}
  This code could be optimized using \textcolor{blue}{shared memory}
\end{frame}
%%%%%
\begin{frame}{Use shared memory to reduce non-coalesced memory: example}
  \begin{figure}
    \includegraphics[scale=0.08]{img/transposeFullCoalesced.png}
  \end{figure}
\end{frame}
%%%%%%%%
% \begin{frame}{(+) Tiling: effective use of shared memory in GEMM}
  
% \end{frame}
%%%%%%%%
% \subsection{Resolving race condition on CUDA}
% %%%%%
% \begin{frame}{To avoid race conditions: barrier synchronization}

% \end{frame}
% %%%%%
% \begin{frame}{To avoid race conditions: Cooperative groups}

% \end{frame}
% %%%%%%%%
% \begin{frame}{(Reluctantly) resolve race conditions: Atomic accumulations}

% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Optimize CUDA program}
% 目次
% \begin{frame}{Contents}
%    \begin{enumerate}[<+->]   % <+-> で 1 行ずつ出現
%    \item \textcolor{gray}{What is CUDA?: Introduction}
%    \item \textcolor{gray}{A CUDA program for beginners}
%    \item \textcolor{gray}{How CUDA works}
%    \item Optimize CUDA program
%    \begin{enumerate}
%      \item How to measure performance?
%      \item Choosing a good block size for performance
%      \item Using shared memory effectively
%      \item Example: matrix multiplication
%    \end{enumerate}
%    \item \textcolor{gray}{Practice Problems}
%  \end{enumerate}
% \end{frame}
\begin{frame}
    \frametitle{Contents}
    \linespread{0.6}\selectfont
    \tableofcontents[currentsection, hideothersubsections]
\end{frame}
%%%%%%%%
\subsection{How to measure performance}
%%%%%
\begin{frame}[fragile]{How to measure performance: APIs}
\vspace{-\baselineskip}
\begin{columns}
\column{0.5\textwidth}
\vspace{-\baselineskip}

If you write a CUDA code, don't forget to check the calculation result and the performance improvement!
\begin{itemize}
  \item To measure time of kernel execution from Host code: \lstinline|cudaEventRecord();|
\end{itemize}
\begin{lstlisting}[language=CUDA, basicstyle=\ttfamily\tiny]
    cudaEvent_t start, stop;
    CUDA_CHECK(cudaEventCreate(&start));
    CUDA_CHECK(cudaEventCreate(&stop));
    
    CUDA_CHECK(cudaEventRecord(start));
    kernel<<<grid, block>>>(...);
    CUDA_CHECK(cudaEventRecord(stop));
\end{lstlisting}
\column{0.5\textwidth}
\begin{itemize}
  \item To measure time of operation on GPU from the device code:\lstinline|clock64();|
\end{itemize}
\begin{lstlisting}[language=CUDA, basicstyle=\ttfamily\tiny]
    typedef struct {
        uint64_t mma_start;
        uint64_t mma_end;
    } DeviceTimingInfo;

    __global__ void kernel(...
        , DeviceTimingInfo *timing_info) {
      bool is_measuring_thread = (threadIdx.x == 0 
              && blockIdx.x == 0 && blockIdx.y == 0);
      if (is_measuring_thread) {
          timing_info->mma_start = clock64();
      }
      // target operation
      if (is_measuring_thread) {
          timing_info->mma_end = clock64();
      }
    }
\end{lstlisting}
\end{columns}
\end{frame}
%%%%%
\begin{frame}[fragile]{How to measure performance: Tips}
When you measure time, you have to take care of...
\begin{itemize}
  \item Don't forget to do warm-ups in order to remove the effect of minor faults or device initialization
  \item Should iterate the execution, and calculate the average of them
  \item Insert synchronization (eg. between Host and Device, threads on a SM) as much as needed.
\end{itemize}
To interpret the result, it is important to
\begin{itemize}
  \item understand what the time really consists of
  \begin{itemize}
    \item Is it just a computation or a computation with a memory access? Think of \lstinline|c[i] += A[j]*B[k]|
    \item Does it include synchronization overhead or not? (This overhead cannot be ignored!)
  \end{itemize}
\end{itemize}
\end{frame}
%%%%%%%%%
%\begin{frame}{Matmul program (naive implementation)}
%
%\end{frame}
%%%%%%%%%
%\begin{frame}{Choosing a good block size, thread dim}
%
%\end{frame}
%%%%%%%%
%\begin{frame}{Using shared memory, cache effectively}
%
%\end{frame}
%%%%%%%%
\subsection{Using Tensore Core}
%%%%%
\begin{frame}[fragile]{Using Tensor Core}
\begin{columns}
\column{0.55\textwidth}
\begin{itemize}
  \item \textbf{Tensor Cores} are specialized high-performance compute cores for matrix multiply and accumulate (MMA) math operations $C=A^TB+C$.
  \begin{itemize}
    \item \lstinline|A| takes the shape of $(M,K)$, \lstinline|B| $(K,N)$, \lstinline|C| $(M,N)$
  \end{itemize}
  \item It is expected that it consists of 16 stages of FMA units with a parallel degree of $16\times16$.
  \begin{itemize}
    \item This explains why they support $(M,N,K)=(16,16,16), (32,8,16), (8,32,16)$
  \end{itemize}
\end{itemize}
\column{0.45\textwidth}
\begin{figure}
  \includegraphics[scale=0.3]{img/FP16TensorCore.png}
  \caption{The image of Tensor Core}
\end{figure}
\end{columns}
\end{frame}
%%%%%%%%
\begin{frame}[fragile]{Using Tensor Core: Step by Step}
Four steps are required to use \textbf{Tensor Core} with \lstinline|<mma.h>|
\begin{columns}
\column{0.7\textwidth}
\begin{enumerate}
  \item reserve RF for containing a section of a matrix distributed across all threads in a warp.
  \begin{itemize}
    \item \lstinline|wmma::fragment<Use, M, N, K, Type, Layout_type> fragment_name;|
    \item \lstinline|Use|: use of the fragment, \lstinline|wmma::matrix_a, wmma::matrix_b, wmma::accumulator|
    \item \lstinline|M, N, K|: dimension of operation, \lstinline|matrix_a| should be $M\times K$, \lstinline|matrix_b| $K\times N$, \lstinline|accumulator| $M\times N$
    \item \lstinline|Type| takes one of \lstinline|half|, \lstinline|float|
    \item \lstinline|Layout|: must be specified for \lstinline|matrix_a|, \lstinline|matrix_b|
    \begin{itemize}
      \item if the columns extend in the z direction, use \lstinline|col_major|
      \item otherwise (i.e., the rows extends in the z direction), use \lstinline|row_major|
    \end{itemize}
  \end{itemize}
\end{enumerate}
\column{0.35\textwidth}
\vspace{-2\baselineskip}
\begin{figure}
  \includegraphics[scale=0.1]{img/mma_warp.png}
  \caption{The image of Tensor Core}
\end{figure}
\end{columns}
\end{frame}
%%%%%%
\begin{frame}[fragile]{Using Tensor Core: Step by Step}
\begin{enumerate}\setcounter{enumi}{1}
  \item fill a matrix fragment (mainly \lstinline|wmma::accumulator|) with a constant value, with \lstinline|wmma::fill_fragment|
  \item waits until all warp lanes(threads) have arrived at, and then loads the matrix fragment from memory onto RF region that has been secure in the Step1
  \begin{itemize}
    \item \lstinline|wmma::load_matrix_sync(fragment_name, mat_ptr, ldm, Layout_type);|
    \item \lstinline|mat_ptr|: the top address of the matrix (the submatrix of matrix)\footnote{In C/C++, \lstinline|A+offset| is equivalent to \lstinline|A+(offset*sizeof(item))|}
    \item \lstinline|ldm|: the number of elements you want to load at once (eg. the number of columns if you load a row)
    \item \lstinline|Layout_type|: the one for \lstinline|matrix_a| and \lstinline|matrix_b| is inferred (you can skip this), but must specify the one for an \lstinline|accumulator| (most of the time, \lstinline|wmma::mem_row_major|)
  \end{itemize}
  % \item Waits until all warp lanes have arrived at, then performs the warp-synchronous MMA operation
  % \begin{itemize}
  %   \item \lstinline|wmma::mma_sync(d_frag, a_frag, b_frag, c_frag [, satf]);|
  %   \item \lstinline|d_frag| and \lstinline|c_frag| could be the same, if \lstinline|satf| is \lstinline|true|, saturation to finite value will be applied
  % \end{itemize}
  % \item Syncs, then stores the matrix fragment to memory, the arguments are the same with \lstinline|load_matrix_sync|
\end{enumerate}
\end{frame}
%%%%%
\begin{frame}[fragile]{Using Tensor Core: Step by Step}
\begin{enumerate}\setcounter{enumi}{3}
  % \item fill a matrix fragment (mainly \lstinline|wmma::accumulator|) with a constant value, with \lstinline|wmma::fill_fragment|
  % \item waits until all warp lanes(threads) have arrived at, and then loads the matrix fragment from memory onto RF region that has been secure in the Step1
  % \begin{itemize}
  %   \item \lstinline|wmma::load_matrix_sync(fragment_name, mat_ptr, ldm, Layout_type);|
  %   \item \lstinline|mat_ptr|: the top address of the matrix (the submatrix of matrix)\footnote{In C/C++, \lstinline|A+offset| is equivalent to \lstinline|A+(offset*sizeof(item))|}
  %   \item \lstinline|ldm|: the number of elements you want to load at once (eg. the number of columns if you load a row)
  %   \item \lstinline|Layout_type|: the one for \lstinline|matrix_a| and \lstinline|matrix_b| is inferred (you can skip this), but must specify the one for an \lstinline|accumulator| (most of the time, \lstinline|wmma::mem_row_major|)
  % \end{itemize}
  \item Waits until all warp lanes have arrived at, then performs the warp-synchronous MMA operation
  \begin{itemize}
    \item \lstinline|wmma::mma_sync(d_frag, a_frag, b_frag, c_frag [, satf]);|
    \item \lstinline|d_frag| and \lstinline|c_frag| could be the same, if \lstinline|satf| is \lstinline|true|, saturation to finite value will be applied
  \end{itemize}
  \item Syncs, then stores the matrix fragment to memory, the arguments are the same with \lstinline|load_matrix_sync|
\end{enumerate}
\end{frame}
%%%%%
\begin{frame}[fragile]{Using Tensor Core: Code Template}
\begin{lstlisting}[language=CUDA, basicstyle=\ttfamily\tiny]
#include <mma.h>
using namespace nvcuda;

__global__ void wmma_ker(half *a, half *b, float *c) {
   // Declare the fragments
   wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::col_major> a_frag;
   wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::row_major> b_frag;
   wmma::fragment<wmma::accumulator, 16, 16, 16, float> c_frag;

   // Initialize the output to zero
   wmma::fill_fragment(c_frag, 0.0f);

   // Load the inputs
   wmma::load_matrix_sync(a_frag, a, 16);
   wmma::load_matrix_sync(b_frag, b, 16);

   // Perform the matrix multiplication
   wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);

   // Store the output
   wmma::store_matrix_sync(c, c_frag, 16, wmma::mem_row_major);
}
\end{lstlisting}
\end{frame}
%%%%%
\begin{frame}[fragile]{Observation of tensorcore matrix multiplication}
\begin{figure}
  \includegraphics[scale=0.25]{img/tensorcoreTotalComputeTimeComparison.png}
  \caption{Tensor Core Total Kernel Execution Time Comparison}
\end{figure}
\vspace{-\baselineskip}

\ulhref{https://github.com/gunnersgoestocl/cuda-introduction/blob/main/tensorcore/}{Codes} 
{\small
You can do \lstinline|qsub ./run.sh|, and then \lstinline|./vis.sh YYYYMMDDHHMMSS| to visualize (Timestamp is available from the name of  \lstinline|result/result_**.txt|)}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Practice Problems}
%%%%%%%%%
% 目次
% \begin{frame}{Contents}
%  \begin{enumerate}   % <+-> で 1 行ずつ出現
%    \item \textcolor{gray}{What is CUDA?: Introduction}
%    \item \textcolor{gray}{A CUDA program for beginners}
%    \item \textcolor{gray}{How CUDA works}
%    \item \textcolor{gray}{Optimize CUDA program}
%    \item Practice Problems
%    \begin{enumerate}
%      \item Problem: accelerate the Transformer block inference
%      \item (Option) Mundane problems
%    \end{enumerate}
%  \end{enumerate}
% \end{frame}
\begin{frame}
    \frametitle{Contents}
    \linespread{0.6}\selectfont
    \tableofcontents[currentsection, hideothersubsections]
\end{frame}
%%%%%%%%
\begin{frame}{Practical Problem: accelerate the Transformer block inference}
  \begin{itemize}
    \item Model Architecture
    \begin{enumerate}
      \item \lstinline|Matmul| \lstinline|input_embeddings| with $W_Q, W_K, W_V$ each
      \item \lstinline|Matmul| \lstinline|Transpose(Q)| with \lstinline|K| to get scores \lstinline|S|
      \item \lstinline|Softmax| on \lstinline|Score| to get (normalized) attention scores \lstinline|A|
      \item \lstinline|Matmul| \lstinline|V| with \lstinline|A| to get output \lstinline|O| of the Head
      \item \lstinline|Concat| \lstinline|O| of each head into a single output \lstinline|Os|
      \item \lstinline|Add| \lstinline|input_embeddings| with \lstinline|Os| (Residual Connection) -> calls \lstinline|Oracle| here
      \item \lstinline|LayerNorm| on \lstinline|Oracle| 
      \item \lstinline|Matmul| with Feed Forward's \lstinline|weights|
      \item \lstinline|ReLU|, again \lstinline|Matmul|
    \end{enumerate}
    \item First step: define \lstinline|Kernel|s of each operation
    \item Second step: measure performance, and optimize them
    \item Third step: cooperate a set of operations using Streams(, Devices), Nodes
  \end{itemize}
\end{frame}
%%%%%%%%
\begin{frame}{Further problems ...}
  % list of mundane problems
  \begin{enumerate}
    \item k-NN for vector Database (on huge vectors)
    \item local alignment of DNA arrays: Smith Watermann's DP
    \item Sort on GPU
  \end{enumerate}
\end{frame}
%%%%%%%
\section{Appendices}
% %%%%%
% \begin{frame}{Contents}
%   \begin{enumerate}%[<+->]   % <+-> で 1 行ずつ出現
%     \item \textcolor{gray}{What is CUDA?: Introduction}
%     \item \textcolor{gray}{A CUDA program for beginners}
%     \item \textcolor{gray}{How CUDA works}
%     \item \textcolor{gray}{Optimize CUDA program}
%     \item \textcolor{gray}{Practice Problems}
%     \item Appendices
%   \end{enumerate}
% \end{frame}
\begin{frame}
    \frametitle{Contents}
    \linespread{0.6}\selectfont
    \tableofcontents[currentsection, hideothersubsections]
\end{frame}
%%%%%
\begin{frame}
\begin{itemize}
  \item How to avoid race conditions between threads
  \item Register shared threads intra warp
  \item Cuda Graph
  \item Usage of profiler (nsys, ncu)
\end{itemize}
\end{frame}
%%%%%
\begin{frame}[fragile]{Compute Capability of H100 GPU}
  \begin{figure}
    \includegraphics[scale=0.25]{img/computeCapability.png}
    \caption{NVIDIA H100 white paper}
  \end{figure}
  \ulhref{https://www.advancedclustering.com/wp-content/uploads/2022/03/gtc22-whitepaper-hopper.pdf}{NVIDIA H100 white paper link}
\end{frame}
%%%%%
\begin{frame}[fragile]{PTX code example}
If you want to see the Assembly of Device Code, just run \lstinline|cuobjdump -ptx $(TARGET) > $(TARGET).ptx|.
Basic PTX formats are as follow:
\begin{itemize}
  \item (instruction name).(data type) \%(dst reg number), \%(src1 reg number) [, \%(src2 reg number)]
  \item \lstinline|mov|, arithmetics(\lstinline|add| etc.), memory access (\lstinline|st| etc.,), bit-wise and so on are available 
\end{itemize}
However, some kinds of special instructions needs additional fields like
\lstinline|wmma.load.a.sync.aligned.row.m16n16k16.global.f16 {%r10, %r11, %r12, %r13, %r14, %r15, %r16, %r17}, [%rd15], %r28;|
\end{frame}
%%%%%
\subsection{Hardware implementation of LSU}
%%%%%
\begin{frame}[fragile]{(Auxiliary) Hardware implementation of LSU}
  % \vspace{-1.5\baselineskip}
%  \begin{block}{}
    \begin{lstlisting}[language=verilog, basicstyle=\ttfamily\tiny]
// lsu.v
module lsu(
  input clk, rstd,             // clock
  input [2:0] func_code_in,    // function code (LB, LH, LW, LBU, LHU, SB, SH, SW)
  input [31:0] alu_result_in,  // ALU result (potential load address, store address)
  input [31:0] s_data,         // store data
  ...
  output reg [31:0] l_data,    // data loaded from memory
  ...
);
  reg [31:0] s_word;       // word to store in memory
  reg [3:0] we;            // write enable
  wire [1:0] s_tag = alu_result_in[1:0];       // tag
  wire [13:0] s_base = alu_result_in[15:2];    // base address
  
  ram M_data(
    .clk(clk),  // control signals
    // input
    .we(we),
    .r_addr(l_base_M), 
    .w_addr(s_base),
    .w_data(s_word),
    // output
    .r_data(l_word)
  );
  ...
\end{lstlisting}
%  \end{block}
\end{frame}
%%%%%
\begin{frame}[fragile]%{(Auxiliary) Hardware implementation of CPU's LSU}
  % \vspace{-1.5\baselineskip}
%  \begin{block}{}
    \begin{lstlisting}[language=verilog, basicstyle=\ttfamily\tiny]
  /* M-stage */
  always @(*) begin
    if (is_store == `ENABLE) begin // transform s_data to 4byte s_word as it corresponds to we
      case(func_code_in)
        `SB: begin
          case (s_tag)
            2'b00: begin s_word = {24'b0, s_data[7:0]};  we = 4'b0001; end
            2'b01: begin s_word = {16'b0, s_data[7:0], 8'b0};  we = 4'b0010; end
            2'b10: begin s_word = {8'b0, s_data[7:0], 16'b0};  we = 4'b0100; end
            2'b11: begin s_word = {s_data[7:0], 24'b0};  we = 4'b1000; end
            ...
          endcase
        end
        `SH: begin
          case(s_tag)
            2'b00: begin s_word = {16'b0, s_data[15:0]}; we = 4'b0011; end
            2'b01: begin s_word = {8'b0, s_data[15:0], 8'b0};  we = 4'b0110; end
            2'b10: begin s_word = {s_data[15:0], 16'b0}; we = 4'b1100; end
            ...
          endcase
        end
        `SW: begin 
          s_word = s_data;  we = 4'b1111;
        end
        ...
      endcase
    end else begin s_word = 32'b0; we = 4'b0000; end
  end
  ... // load/store word from/to memory (base = l_addr[31:2]) synchronized with clk
\end{lstlisting}
%  \end{block}
\end{frame}
%%%%%
\begin{frame}[fragile]%{(Auxiliary) Hardware implementation of LSU}
  % \vspace{-1.5\baselineskip}
    \begin{lstlisting}[language=verilog, basicstyle=\ttfamily\tiny]
  /* W-stage */
  always @(*) begin
    if (is_load_W == `ENABLE) begin
      case(func_code_W) // transform l_word to l_data as it corresponds to func_code
        `LB: begin
          case (l_tag_W)
            2'b00: l_data = {{24{l_word[7]}}, l_word[7:0]};
            2'b01: l_data = {{24{l_word[15]}}, l_word[15:8]};
            2'b10: l_data = {{24{l_word[23]}}, l_word[23:16]};
            2'b11: l_data = {{24{l_word[31]}}, l_word[31:24]};
            default: l_data = 32'b0;
          endcase
        end
        `LH: begin
          case (l_tag_W)
            2'b00: l_data = {{16{l_word[15]}}, l_word[15:0]};
            2'b01: l_data = {{16{l_word[23]}}, l_word[23:8]};
            2'b10: l_data = {{16{l_word[31]}}, l_word[31:16]};
            default: l_data = 32'b0;
          endcase
        end
        `LW: begin  l_data = l_word; end // l_addr[1:0] == 2'b00 はコンパイラが保証
        `LBU: begin  ...  end
        `LHU: begin  ...  end
        default l_data = 32'b0;
      endcase
    end else begin l_data = 32'b0; end
  end
\end{lstlisting}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{References}
  \begin{enumerate}\footnotesize
    \item \url{https://docs.nvidia.com/cuda/parallel-thread-execution/}
    \item \url{https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html}
    \item \url{https://resources.nvidia.com/en-us-data-center-overview-mc/en-us-data-center-overview/grace-hopper-superchip-datasheet-partner}
    \item \url{https://docs.nvidia.com/cuda/cuda-runtime-api/index.html}
    \item \url{https://www.nas.nasa.gov/hecc/support/kb/basics-on-nvidia-gpu-hardware-architecture_704.html}
    \item \url{https://developer.nvidia.com/blog/unified-memory-cuda-beginners/}
    \item \url{https://leimao.github.io/blog/CUDA-Coalesced-Memory-Access/}
    \item \url{https://www.nvidia.com/content/pdf/fermi_white_papers/p.glaskowsky_nvidia's_fermi-the_first_complete_gpu_architecture.pdf}
    \item \url{https://developer.nvidia.com/ja-jp/blog/nvidia-hopper-architecture-in-depth/}
    \item \url{https://qiita.com/tarako1889/items/963e8972daa8c490efd4}
    \item \url{https://www.docswell.com/s/fixstars/5RXQJ2-20220623}
  \end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
