# CUDA Programming Challenge

## â€œBruteâ€‘Force kâ€‘Nearest Neighbor (kâ€‘NN) Kernel for a Vectorâ€¯Databaseâ€

---

## 1. Why this problem?

Modern vector databases (Pinecone, Milvus, Weaviate, FAISS, etc.) rely on extremely fast *vector similarity search* to retrieve the topâ€‘K nearest embeddings out of millions or billions. Even highlyâ€‘optimized approximate indexes (IVFâ€‘PQ, HNSW, ScaNN) fall back on an *exact* bruteâ€‘force pass for small candidate lists, reâ€‘ranking, or recall evaluation. On GPUs, this exact pass is often the throughput bottleneck and dominates both **latency** and **energy** costÂ ([NVIDIA Developer][1]).

Designing a highâ€‘performance CUDA kernel for bruteâ€‘force kâ€‘NN is therefore a **biteâ€‘sized yet highâ€‘impact** exercise: it teaches memoryâ€‘bandwidth management, warpâ€‘level primitives, register blocking, and GPUâ€‘centric Topâ€‘K selection, all of which translate directly to realâ€‘world vectorâ€‘database speedupsÂ ([GitHub][2], [vincentfpgarcia.github.io][3]).

---

## 2. Learning goals

| Goal                                         | Why it matters for vector DBs                                                               |
| -------------------------------------------- | ------------------------------------------------------------------------------------------- |
| **Memory coalescing & sharedâ€‘memory tiling** | Distance computation is a pure memoryâ€‘bandwidth problem; poor layout wastes >50â€¯% of GPU BW |
| **Warpâ€‘level reduction & Topâ€‘K selection**   | kâ€‘NN requires a *partial* sort (kâ‰ªN); naÃ¯ve global sorts are 100Ã— slower                    |
| **Mixed precision & tensor cores**           | L2 / cosine distances tolerate FP16, doubling effective BW on Ampere+                       |
| **Rooflineâ€‘based performance analysis**      | Learn to attribute bottlenecks to compute vs. memory and guide optimizations                |

---

## 3. Problem statement

> **Implement and optimize a CUDA kernel that, given**
> â€“â€¯A database matrix **D** âˆˆ â„<sup>NÃ—d</sup> (Nâ€¯â‰¤â€¯1â€¯024â€¯000, dâ€¯=â€¯128) stored rowâ€‘major in GPU global memory
> â€“â€¯A query matrix **Q** âˆˆ â„<sup>QÃ—d</sup> (Qâ€¯â‰¤â€¯4096)
> **returns the indices and squaredâ€‘L2 distances of the kâ€¯=â€¯10 nearest neighbors for every query**, in *ascending* distance order.
>
> You must deliver:
>
> 1. A **baseline** CPU (singleâ€‘thread) reference implementation (for correctness & speedup measurement).
> 2. A **naÃ¯ve** CUDA kernel (one threadâ€¯=â€¯one distance) that is functionally correct.
> 3. At least **two incremental optimizations**, chosen from the menu below, that together achieve â‰¥Â **250Ã— speedup** over the CPU baseline on a recent NVIDIA RTX/Ampere or newer GPU.
> 4. A *oneâ€‘page* roofline analysis summarizing where each optimization moves the kernel on the roofline plot.

### Allowed optimization menu (pick at least two)

1. **Tiled sharedâ€‘memory loads** of query and database tiles (e.g., 32Ã—128 floats) to turn scattered global reads into coalesced 128â€‘byte transactions.
2. **Vectorized I/O** (`float4`/`__half2`) and *mixedâ€‘precision* accumulation using tensorâ€‘core MMA or CUTLASS GEMM, exploiting the distanceâ€‘asâ€‘GEMM identityÂ ([NVIDIA Developer][1]).
3. **Warpâ€‘level partial reduction**: use `__shfl_down_sync` or cooperative groups to keep a perâ€‘warp Topâ€‘K heap in registers, pushing only the final candidates to shared/global memory.
4. **Blockâ€‘tiling + register blocking**: each block computes a Q\_tile Ã— D\_tile distance subâ€‘matrix; each thread accumulates four distances in registers to increase arithmetic intensity.
5. **Asynchronous memory copies** (`cp.async`) on Hopper/Ada architectures to overlap globalâ€‘toâ€‘shared transfers with compute.

---

## 4. Reference interface (header)

```cpp
// distances_out: Q Ã— k (rowâ€‘major),  float32
// indices_out  : Q Ã— k (rowâ€‘major),  int32
void gpu_knn(const float* __restrict__ d_database,
             const float* __restrict__ d_queries,
             int32_t*    __restrict__ d_indices_out,
             float*      __restrict__ d_distances_out,
             int N, int Q, int d, int k /*=10*/);
```

Compile with `nvcc -arch=sm_80 -O3 -lineinfo`.

---

## 5. Evaluation protocol

1. **Correctness**: Mismatch â‰¤â€¯1â€¯eâ€‘4 with CPU reference per distance (allow FP16 rounding).
2. **Performance**: Report

   * *Throughput*â€¯=â€¯`(NÂ·Q) / elapsed_time` distancesâ€¯Â·â€¯sâ»Â¹
   * *Endâ€‘toâ€‘end latency* for Qâ€¯=â€¯1024, 4096
   * *SM occupancy* and achieved DRAM BW (from Nsight Compute)
3. **Scalability experiment**: Plot throughput vs. N on logâ€‘scale up to 1â€¯M. Verify O(N) behavior.
4. **Reflection**: Â½â€‘page explaining which bottleneck you hit first and how each optimization attacked it.

---

## 6. Hints & starter ideas

* **Distanceâ€¯=â€¯GEMM** trick: â€–qâˆ’dâ€–Â²â€¯=â€¯â€–qâ€–Â²+â€–dâ€–Â²âˆ’2â€¯qáµ€d â†’ preâ€‘compute â€–dâ€–Â² once, reuse across queries; inner products become a QÃ—N GEMM that tensor cores can accelerateÂ ([arXiv][4]).
* Use **`cub::WarpReduce`** or **`thrust::topk`â€‘style reduction** for efficient perâ€‘warp Topâ€‘K.
* Optimal tile sizes often align with **memoryâ€‘controller burst length** (128â€¯B) and **warp size** (32). Start with 128 threads/block, 4 warps, 16Ã—16 tile.
* Batch queries so that **Q\_tile Â· d** fits into shared memory (48â€“100â€¯KiB per SM on Ampere).
* Study FAISS GPU kernels for proven parameter choicesÂ ([Engineering at Meta][5]).

---

## 7. Extension paths (optional, for extra credit)

| Path                     | Idea                                                                                                                                                   |
| ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Indexing**             | Add a coarse *IVF* layer: cluster database into 1024 centroids (kâ€‘means on CPU), search only the 4 nearest cells on GPU. Measure recall\@10 vs. speed. |
| **Precision vs. Recall** | Compare FP16 vs. FP32 distances on a real embedding set (e.g., GloVe vectors). Report recall degradation, if any.                                      |
| **Multiâ€‘GPU sharding**   | Partition D across two GPUs; implement NCCL allâ€‘gather of Topâ€‘K, show nearâ€‘linear speedupÂ ([GitHub][2]).                                               |
| **Inâ€‘GPU paging**        | Stream database chunks from host to device with `cudaMemcpyAsync` + compute/transfer overlap for Nâ€¯>â€¯10â€¯M.                                             |

---

## 8. Deliverables checklist

* [ ] `knn_cpu.cpp`Â +Â `knn_gpu.cu` source files
* [ ] `benchmark.py` or `run.sh` with reproducible timing
* [ ] Nsight Compute report (`.ncu-rep`) highlighting memory vs. compute utilization
* [ ] PDF writeâ€‘up (â‰¤â€¯4â€¯pages) with roofline plot and optimization narrative

Good luck, and enjoy squeezing every last GFLOP out of your GPU! ğŸ¯

[1]: https://developer.nvidia.com/blog/accelerated-vector-search-approximating-with-nvidia-cuvs-ivf-flat/?utm_source=chatgpt.com "Accelerated Vector Search: Approximating with NVIDIA cuVS ..."
[2]: https://github.com/facebookresearch/faiss/wiki/Faiss-on-the-GPU?utm_source=chatgpt.com "Faiss on the GPU Â· facebookresearch/faiss Wiki - GitHub"
[3]: https://vincentfpgarcia.github.io/data/Garcia_2010_ICIP.pdf?utm_source=chatgpt.com "[PDF] k-nearest neighbor search: fast gpu-based implementations"
[4]: https://arxiv.org/pdf/1702.08734?utm_source=chatgpt.com "[PDF] Billion-scale similarity search with GPUs - arXiv"
[5]: https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/?utm_source=chatgpt.com "Faiss: A library for efficient similarity search - Engineering at Meta"
